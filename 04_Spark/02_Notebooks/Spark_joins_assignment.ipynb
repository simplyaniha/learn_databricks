{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "Xz8INA7rPLao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework Assignment: Joins in Apache Spark\n",
        "\n",
        "## Objective\n",
        "This assignment aims to familiarize you with joins in Apache Spark, a powerful distributed data processing engine. You will learn about the different types of joins available in Spark, their use cases, and how to implement them using PySpark (the Python API for Spark). We will use the famous MovieLens dataset, which is widely used in data science and machine learning for recommendation systems. This dataset allows us to demonstrate joins by combining information about movies and user ratings.\n",
        "\n",
        "By the end of this assignment, you should be able to:\n",
        "- Understand and differentiate between various join types in Spark.\n",
        "- Load datasets into Spark DataFrames.\n",
        "- Perform joins and analyze the results.\n",
        "- Handle common issues like data skew or null values in joins.\n",
        "\n",
        "## Prerequisites\n",
        "- Install Apache Spark on your local machine or use a cloud-based environment like Databricks Community Edition (free tier) or Google Colab with PySpark installed.\n",
        "- Python knowledge and familiarity with PySpark basics (e.g., creating SparkSessions and DataFrames).\n",
        "- Download the MovieLens dataset as instructed below.\n",
        "\n",
        "## Dataset: MovieLens 100K\n",
        "We will use the MovieLens 100K dataset, which contains 100,000 ratings from 943 users on 1,682 movies. It includes two main files:\n",
        "- `movies.csv`: Contains movie details (movieId, title, genres).\n",
        "- `ratings.csv`: Contains user ratings (userId, movieId, rating, timestamp).\n",
        "\n",
        "This dataset is perfect for joins because we can combine movie information with ratings based on the common `movieId` column.\n",
        "\n",
        "### Instructions to Download and Use the Dataset\n",
        "1. Visit the official GroupLens website: [https://grouplens.org/datasets/movielens/100k/](https://grouplens.org/datasets/movielens/100k/).\n",
        "2. Download the \"ml-100k.zip\" file (it's free and no registration is required).\n",
        "3. Unzip the file to a directory on your local machine (e.g., `/path/to/ml-100k/`).\n",
        "4. The relevant files are:\n",
        "   - `u.item` (rename to `movies.csv` for simplicity; format it as CSV with columns: movieId|title|genres – you may need to process it slightly).\n",
        "   - `u.data` (rename to `ratings.csv`; format: userId|movieId|rating|timestamp, tab-separated – convert to CSV if needed).\n",
        "5. In your PySpark code, load the files using `spark.read.csv()` with appropriate options (e.g., `header=False`, `sep='|'` for movies, `sep='\\t'` for ratings).\n",
        "   - Example loading code:\n",
        "     ```python\n",
        "     from pyspark.sql import SparkSession\n",
        "\n",
        "     spark = SparkSession.builder.appName(\"JoinsAssignment\").getOrCreate()\n",
        "\n",
        "     # Load movies (assuming renamed and formatted as CSV)\n",
        "     movies_df = spark.read.csv(\"/path/to/ml-100k/movies.csv\", header=False, sep=\"|\") \\\n",
        "         .toDF(\"movieId\", \"title\", \"genres\")  # Adjust columns as needed\n",
        "\n",
        "     # Load ratings\n",
        "     ratings_df = spark.read.csv(\"/path/to/ml-100k/ratings.csv\", header=False, sep=\"\\t\") \\\n",
        "         .toDF(\"userId\", \"movieId\", \"rating\", \"timestamp\")\n",
        "     ```\n",
        "\n",
        "If you encounter issues with file formats, use pandas to preprocess and save as proper CSV before loading into Spark.\n",
        "\n",
        "## Background: Joins in Spark\n",
        "Joins in Spark are operations that combine two or more DataFrames based on a related column (join key). Spark supports distributed joins, which are efficient for large datasets but can be computationally expensive if not optimized (e.g., using broadcast joins for small datasets).\n",
        "\n",
        "### Types of Joins in Spark\n",
        "Spark provides several join types via the `DataFrame.join()` method. The syntax is:\n",
        "```python\n",
        "result_df = left_df.join(right_df, on=\"join_key_column\", how=\"join_type\")\n"
      ],
      "metadata": {
        "id": "02qg6MxMP2L2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESsBMmkgPHA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "XLCwMi9fPH5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApplication\") \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "AQV1X13G_D9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQSiab3j-wQs",
        "outputId": "b622edd8-26eb-4646-c1bd-ff69c0aa20c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 19:07:41--  https://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/2022.csv.gz\n",
            "Resolving noaa-ghcn-pds.s3.amazonaws.com (noaa-ghcn-pds.s3.amazonaws.com)... 52.217.89.204, 3.5.20.65, 54.231.225.57, ...\n",
            "Connecting to noaa-ghcn-pds.s3.amazonaws.com (noaa-ghcn-pds.s3.amazonaws.com)|52.217.89.204|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97608893 (93M) [application/octet-stream]\n",
            "Saving to: ‘2022.csv.gz’\n",
            "\n",
            "2022.csv.gz         100%[===================>]  93.09M  45.4MB/s    in 2.0s    \n",
            "\n",
            "2025-10-07 19:07:43 (45.4 MB/s) - ‘2022.csv.gz’ saved [97608893/97608893]\n",
            "\n",
            "--2025-10-07 19:07:43--  https://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
            "Resolving noaa-ghcn-pds.s3.amazonaws.com (noaa-ghcn-pds.s3.amazonaws.com)... 52.216.206.243, 52.217.113.9, 3.5.30.182, ...\n",
            "Connecting to noaa-ghcn-pds.s3.amazonaws.com (noaa-ghcn-pds.s3.amazonaws.com)|52.216.206.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11150588 (11M) [application/octet-stream]\n",
            "Saving to: ‘ghcnd-stations.txt’\n",
            "\n",
            "ghcnd-stations.txt  100%[===================>]  10.63M  15.7MB/s    in 0.7s    \n",
            "\n",
            "2025-10-07 19:07:44 (15.7 MB/s) - ‘ghcnd-stations.txt’ saved [11150588/11150588]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/2022.csv.gz\n",
        "!wget https://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when, substring, expr, to_date\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Colab\").getOrCreate()"
      ],
      "metadata": {
        "id": "bXWY8i-nFpDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load station metadata (fixed-width text)\n",
        "stations_df = spark.read.text(\"ghcnd-stations.txt\").select(\n",
        "    expr(\"substring(value, 1, 11) AS station\"),\n",
        "    expr(\"substring(value, 13, 8) AS latitude\").cast(\"float\"),\n",
        "    expr(\"substring(value, 22, 9) AS longitude\").cast(\"float\"),\n",
        "    expr(\"substring(value, 32, 6) AS elevation\").cast(\"float\"),\n",
        "    expr(\"substring(value, 42, 30) AS name\")\n",
        ")\n",
        "\n",
        "# Load observations CSV (no header)\n",
        "raw_df = spark.read.option(\"header\", False).csv(\"2022.csv.gz\").toDF(\n",
        "    \"station\", \"date\", \"element\", \"value\", \"mflag\", \"qflag\", \"sflag\", \"obstime\"\n",
        ")\n",
        "\n",
        "# Filter for TAVG, convert value to degrees C, parse date\n",
        "tavg_df = raw_df.where((col(\"element\") == \"TAVG\") & col(\"qflag\").isNull()).select(\n",
        "    \"station\",\n",
        "    to_date(col(\"date\"), \"yyyyMMdd\").alias(\"date\"),\n",
        "    (col(\"value\").cast(\"float\") / 10).alias(\"TAVG\")\n",
        ")\n",
        "\n",
        "# Add 'code' column ('F' for US stations, 'C' otherwise) and convert TAVG to F where needed\n",
        "tavg_df = tavg_df.withColumn(\"code\", when(substring(col(\"station\"), 1, 2) == \"US\", \"F\").otherwise(\"C\"))\n",
        "tavg_df = tavg_df.withColumn(\"TAVG\", when(col(\"code\") == \"F\", (col(\"TAVG\") * 9/5) + 32).otherwise(col(\"TAVG\")))\n",
        "\n",
        "# Join with stations\n",
        "processed_df = tavg_df.join(stations_df, on=\"station\", how=\"left\")\n",
        "\n",
        "# Save as Parquet (this creates the 'stationData.parquet' directory with Parquet parts)\n",
        "processed_df.write.mode(\"overwrite\").parquet(\"stationData.parquet\")"
      ],
      "metadata": {
        "id": "0KrmD0hlErDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "largeDF = spark.read \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"inferSchema\", True) \\\n",
        "    .parquet(\"stationData.parquet\") \\\n",
        "    .limit(2000)"
      ],
      "metadata": {
        "id": "DEW8xB8xErA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "largeDF.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8ASjs21Eqjr",
        "outputId": "68d4b340-c30d-4652-c2c1-2d713664548d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+----+----+--------+---------+---------+--------------------+\n",
            "|    station|      date|TAVG|code|latitude|longitude|elevation|                name|\n",
            "+-----------+----------+----+----+--------+---------+---------+--------------------+\n",
            "|AEM00041194|2022-01-01|21.1|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-02|23.4|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-03|24.1|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-04|22.7|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-05|21.4|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-06|20.8|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-07|19.7|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-08|19.7|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-09|19.9|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "|AEM00041194|2022-01-10|20.4|   C|  25.255|   55.364|     10.4|DUBAI INTL       ...|\n",
            "+-----------+----------+----+----+--------+---------+---------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "txqJumi7_DZu"
      }
    }
  ]
}